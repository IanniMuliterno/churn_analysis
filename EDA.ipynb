{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the problem\n",
    "- Objective: Determine why and when customers are leaving.\n",
    "- Business Impact: Assess how churn impacts revenue and long-term growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from scipy.stats import skew,kurtosis, pearsonr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitorid</th>\n",
       "      <th>ses_rec</th>\n",
       "      <th>ses_rec_avg</th>\n",
       "      <th>ses_rec_sd</th>\n",
       "      <th>ses_rec_cv</th>\n",
       "      <th>user_rec</th>\n",
       "      <th>ses_n</th>\n",
       "      <th>ses_n_r</th>\n",
       "      <th>int_n</th>\n",
       "      <th>int_n_r</th>\n",
       "      <th>...</th>\n",
       "      <th>int_cat16_n</th>\n",
       "      <th>int_cat17_n</th>\n",
       "      <th>int_cat18_n</th>\n",
       "      <th>int_cat19_n</th>\n",
       "      <th>int_cat20_n</th>\n",
       "      <th>int_cat21_n</th>\n",
       "      <th>int_cat22_n</th>\n",
       "      <th>int_cat23_n</th>\n",
       "      <th>int_cat24_n</th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.935800e+04</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.072986e+05</td>\n",
       "      <td>15.454840</td>\n",
       "      <td>11.231611</td>\n",
       "      <td>2.711961</td>\n",
       "      <td>-0.020100</td>\n",
       "      <td>33.822947</td>\n",
       "      <td>3.366445</td>\n",
       "      <td>0.172372</td>\n",
       "      <td>6.716277</td>\n",
       "      <td>1.720975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955792</td>\n",
       "      <td>0.773714</td>\n",
       "      <td>0.382977</td>\n",
       "      <td>0.732424</td>\n",
       "      <td>0.503343</td>\n",
       "      <td>0.447020</td>\n",
       "      <td>2.102577</td>\n",
       "      <td>0.038130</td>\n",
       "      <td>0.099579</td>\n",
       "      <td>0.885591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.072098e+05</td>\n",
       "      <td>9.184645</td>\n",
       "      <td>18.162743</td>\n",
       "      <td>6.583917</td>\n",
       "      <td>0.917701</td>\n",
       "      <td>25.237703</td>\n",
       "      <td>7.380573</td>\n",
       "      <td>0.372614</td>\n",
       "      <td>38.528882</td>\n",
       "      <td>1.455885</td>\n",
       "      <td>...</td>\n",
       "      <td>6.086722</td>\n",
       "      <td>5.003517</td>\n",
       "      <td>4.569604</td>\n",
       "      <td>4.977989</td>\n",
       "      <td>3.259194</td>\n",
       "      <td>3.873684</td>\n",
       "      <td>16.273213</td>\n",
       "      <td>0.593681</td>\n",
       "      <td>1.135149</td>\n",
       "      <td>0.318311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.700000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.532920e+05</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.100910e+05</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.060355e+06</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.638646</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.407573e+06</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>11.525121</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5549.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>481.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>2282.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          visitorid       ses_rec   ses_rec_avg    ses_rec_sd    ses_rec_cv  \\\n",
       "count  4.935800e+04  49358.000000  49358.000000  49358.000000  49358.000000   \n",
       "mean   7.072986e+05     15.454840     11.231611      2.711961     -0.020100   \n",
       "std    4.072098e+05      9.184645     18.162743      6.583917      0.917701   \n",
       "min    3.700000e+01      0.000000      0.000000      0.000000     -1.000000   \n",
       "25%    3.532920e+05      7.000000      0.000000      0.000000     -1.000000   \n",
       "50%    7.100910e+05     16.000000      2.250000      0.000000      0.000000   \n",
       "75%    1.060355e+06     23.000000     14.250000      1.000000      0.638646   \n",
       "max    1.407573e+06     31.000000     99.000000     47.500000     11.525121   \n",
       "\n",
       "           user_rec         ses_n       ses_n_r         int_n       int_n_r  \\\n",
       "count  49358.000000  49358.000000  49358.000000  49358.000000  49358.000000   \n",
       "mean      33.822947      3.366445      0.172372      6.716277      1.720975   \n",
       "std       25.237703      7.380573      0.372614     38.528882      1.455885   \n",
       "min        0.000000      2.000000     -1.000000      2.000000      1.000000   \n",
       "25%       16.000000      2.000000      0.060606      2.000000      1.000000   \n",
       "50%       26.000000      2.000000      0.090909      3.000000      1.250000   \n",
       "75%       46.000000      3.000000      0.166667      6.000000      2.000000   \n",
       "max       99.000000    475.000000     18.000000   5549.000000     59.000000   \n",
       "\n",
       "       ...   int_cat16_n   int_cat17_n   int_cat18_n   int_cat19_n  \\\n",
       "count  ...  49358.000000  49358.000000  49358.000000  49358.000000   \n",
       "mean   ...      0.955792      0.773714      0.382977      0.732424   \n",
       "std    ...      6.086722      5.003517      4.569604      4.977989   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...    576.000000    445.000000    481.000000    564.000000   \n",
       "\n",
       "        int_cat20_n   int_cat21_n   int_cat22_n   int_cat23_n   int_cat24_n  \\\n",
       "count  49358.000000  49358.000000  49358.000000  49358.000000  49358.000000   \n",
       "mean       0.503343      0.447020      2.102577      0.038130      0.099579   \n",
       "std        3.259194      3.873684     16.273213      0.593681      1.135149   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      2.000000      0.000000      0.000000   \n",
       "max      317.000000    420.000000   2282.000000     54.000000    105.000000   \n",
       "\n",
       "       target_class  \n",
       "count  49358.000000  \n",
       "mean       0.885591  \n",
       "std        0.318311  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ecom-user-churn-data.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deal with duplicates, handle missing values and anomalies.\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate analysis\n",
    "- For Skewness:\n",
    "    - A skewness close to 0 indicates a symmetrical distribution.\n",
    "    - A skewness greater than 1 or less than -1 indicates a highly skewed distribution.\n",
    "    - A skewness between -1 and -0.5 or between 0.5 and 1 indicates moderate skewness.\n",
    "- For Kurtosis:\n",
    "    - A kurtosis greater than 3 indicates a leptokurtic distribution. Traditional interpretations would subtract 3 (excess kurtosis), so a value greater than 0 in this excess kurtosis indicates more outliers than the normal distribution.\n",
    "    - A kurtosis less than 3 indicates a platykurtic distribution. With excess kurtosis (kurtosis - 3), a value less than 0 indicates fewer outliers. \n",
    "    \n",
    "These metrics are valuable for data preprocessing in machine learning. Highly skewed or kurtotic data may need transformation, such as logarithmic, square root, or box-cox transformation, to meet the assumptions of various statistical models and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#univariate analysis\n",
    "def univ_analysis(x):\n",
    "\n",
    "    \"\"\"\n",
    "    get quartiles, skewness, kurtosis and sparseness.\n",
    "    \"\"\"\n",
    "    distribution_stats = [np.quantile(x, [0,.25,.5,.75,1]), skew(x), kurtosis(x),round((1 - (np.count_nonzero(x)/len(x))),4)]\n",
    "    return distribution_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(univ_analysis).to_csv('df_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicolinearity \n",
    "\n",
    "Here comes a question, the combination of predictor variables can have stronger impact in the response variable even if some variables from that combination have no relevant correlation with the response variable? \n",
    "\n",
    "The answer is, yes! Some models might underperform if we just take away variables, so let's be careful about it and perform two kinds of multicolinear analysis.\n",
    "\n",
    "1. pearsons correlation : straightforward and maintain the interpretability of the model but might lead to the loss of some important data\n",
    "2. PCA:are best when reduction of dimensions with minimal loss of information is needed.  \n",
    "\n",
    "PCA will indicate which variable to keep when we find high correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IM_py\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "pairs = list(product(df.columns, repeat = 2))\n",
    "corr_ls = []\n",
    "\n",
    "for c in pairs:\n",
    "    pair_corr, corr_pval = pearsonr(df[c[0]],df[c[1]])\n",
    "    corr_ls.append([c[0],c[1],pair_corr,corr_pval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['int_cat15_n','target_class'], axis= 'columns').values\n",
    "\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>rho</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>ses_rec_avg</td>\n",
       "      <td>ses_mo_sd</td>\n",
       "      <td>0.856849</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>user_rec</td>\n",
       "      <td>ses_mo_avg</td>\n",
       "      <td>-0.811061</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>user_rec</td>\n",
       "      <td>ses_mo_sd</td>\n",
       "      <td>0.803804</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>int_n</td>\n",
       "      <td>tran_n</td>\n",
       "      <td>0.859737</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>int_n</td>\n",
       "      <td>rev_sum</td>\n",
       "      <td>0.855076</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>int_n</td>\n",
       "      <td>int_cat22_n</td>\n",
       "      <td>0.936990</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>int_n_r</td>\n",
       "      <td>int_itm_n_avg</td>\n",
       "      <td>0.917397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>int_n_r</td>\n",
       "      <td>ses_len_avg</td>\n",
       "      <td>0.805345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>tran_n</td>\n",
       "      <td>int_n</td>\n",
       "      <td>0.859737</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>tran_n</td>\n",
       "      <td>rev_sum</td>\n",
       "      <td>0.981603</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>tran_n</td>\n",
       "      <td>int_cat22_n</td>\n",
       "      <td>0.830262</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>tran_n_r</td>\n",
       "      <td>major_spend_r</td>\n",
       "      <td>0.807566</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>rev_sum</td>\n",
       "      <td>int_n</td>\n",
       "      <td>0.855076</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>rev_sum</td>\n",
       "      <td>tran_n</td>\n",
       "      <td>0.981603</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>rev_sum</td>\n",
       "      <td>int_cat22_n</td>\n",
       "      <td>0.831504</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>major_spend_r</td>\n",
       "      <td>tran_n_r</td>\n",
       "      <td>0.807566</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>int_itm_n_avg</td>\n",
       "      <td>int_n_r</td>\n",
       "      <td>0.917397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>ses_mo_avg</td>\n",
       "      <td>user_rec</td>\n",
       "      <td>-0.811061</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>ses_mo_sd</td>\n",
       "      <td>ses_rec_avg</td>\n",
       "      <td>0.856849</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>ses_mo_sd</td>\n",
       "      <td>user_rec</td>\n",
       "      <td>0.803804</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>ses_len_avg</td>\n",
       "      <td>int_n_r</td>\n",
       "      <td>0.805345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>int_cat22_n</td>\n",
       "      <td>int_n</td>\n",
       "      <td>0.936990</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>int_cat22_n</td>\n",
       "      <td>tran_n</td>\n",
       "      <td>0.830262</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>int_cat22_n</td>\n",
       "      <td>rev_sum</td>\n",
       "      <td>0.831504</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               from             to       rho  pvalue\n",
       "116     ses_rec_avg      ses_mo_sd  0.856849     0.0\n",
       "262        user_rec     ses_mo_avg -0.811061     0.0\n",
       "263        user_rec      ses_mo_sd  0.803804     0.0\n",
       "402           int_n         tran_n  0.859737     0.0\n",
       "404           int_n        rev_sum  0.855076     0.0\n",
       "437           int_n    int_cat22_n  0.936990     0.0\n",
       "457         int_n_r  int_itm_n_avg  0.917397     0.0\n",
       "463         int_n_r    ses_len_avg  0.805345     0.0\n",
       "498          tran_n          int_n  0.859737     0.0\n",
       "502          tran_n        rev_sum  0.981603     0.0\n",
       "535          tran_n    int_cat22_n  0.830262     0.0\n",
       "553        tran_n_r  major_spend_r  0.807566     0.0\n",
       "596         rev_sum          int_n  0.855076     0.0\n",
       "598         rev_sum         tran_n  0.981603     0.0\n",
       "633         rev_sum    int_cat22_n  0.831504     0.0\n",
       "697   major_spend_r       tran_n_r  0.807566     0.0\n",
       "793   int_itm_n_avg        int_n_r  0.917397     0.0\n",
       "838      ses_mo_avg       user_rec -0.811061     0.0\n",
       "884       ses_mo_sd    ses_rec_avg  0.856849     0.0\n",
       "887       ses_mo_sd       user_rec  0.803804     0.0\n",
       "1087    ses_len_avg        int_n_r  0.805345     0.0\n",
       "2213    int_cat22_n          int_n  0.936990     0.0\n",
       "2215    int_cat22_n         tran_n  0.830262     0.0\n",
       "2217    int_cat22_n        rev_sum  0.831504     0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_df = pd.DataFrame(data= corr_ls,columns=['from','to','rho','pvalue'])\n",
    "corr_df[(abs(corr_df['rho']) > .8 ) & (corr_df['from'] != corr_df['to'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 5\n",
    "pca = PCA(n_comp)\n",
    "princ_components = pca.fit_transform(x)\n",
    "\n",
    "pc_df = pd.DataFrame(data = princ_components, columns=['PC1','PC2','PC3','PC4','PC5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4180977751843673\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 22, 23, 24, 25, 26, 29, 33, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n"
     ]
    }
   ],
   "source": [
    "loadings = pca.components_\n",
    "print(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "important_features = set() \n",
    "\n",
    "for k in range(n_comp):  \n",
    "    important_features.update([i for i in range(len(loadings[k])) if abs(loadings[k][i]) > 0.1])\n",
    "\n",
    "\n",
    "important_features = sorted(list(important_features))\n",
    "\n",
    "print(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7551020408163265"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(important_features))/len(df.columns)\n",
    "#73% of the columns explains only 42% of the variance, that suggests feature enrichment and/or feature engineering are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ses_rec', 'ses_rec_avg', 'ses_rec_sd', 'ses_rec_cv', 'user_rec',\n",
       "       'ses_n', 'ses_n_r', 'int_n', 'int_n_r', 'tran_n', 'tran_n_r', 'rev_sum',\n",
       "       'rev_sum_r', 'major_spend_r', 'int_cat_n_avg', 'int_itm_n_avg',\n",
       "       'ses_mo_avg', 'ses_mo_sd', 'ses_ho_sd', 'ses_len_avg', 'time_to_int',\n",
       "       'time_to_tran', 'int_cat1_n', 'int_cat2_n', 'int_cat5_n', 'int_cat9_n',\n",
       "       'int_cat11_n', 'int_cat12_n', 'int_cat16_n', 'int_cat17_n',\n",
       "       'int_cat18_n', 'int_cat19_n', 'int_cat20_n', 'int_cat21_n',\n",
       "       'int_cat22_n', 'int_cat23_n', 'int_cat24_n'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['int_cat15_n','target_class'], axis= 'columns').columns[important_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementing WOE and IV\n",
    "\n",
    "for bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iv(df2,variable,target):\n",
    "\n",
    "    \n",
    "    lst = []\n",
    "    #This block iterates through each unique value of the specified feature. For each unique value (val), it computes:\n",
    "    #The total number of occurrences of val.\n",
    "    #The count of occurrences where the target variable is 0 (\"churn\").\n",
    "    #The count of occurrences where the target variable is 1 (\"no churn\").\n",
    "    #These counts are appended to the list lst as a sublist, including the feature name and the value being analyzed.~\n",
    "\n",
    "    for i in range(df2[variable].nunique()):\n",
    "        \n",
    "        variable_val = df2[variable].unique()[i]\n",
    "        aux_df = df2[(df2[variable] == variable_val)]\n",
    "        total_ocur = len(aux_df)\n",
    "        total_churn = len(aux_df[(aux_df[target] == 0)])\n",
    "        total_nochurn = len(aux_df[(aux_df[target] == 1)])\n",
    "\n",
    "        lst.append([variable,variable_val,total_ocur,total_churn,total_nochurn])\n",
    "        \n",
    "    #Share: The proportion of observations for each unique value relative to the total number of observations.\n",
    "    #share churn: The proportion of the \"bad\" outcome for each unique value.\n",
    "    #Distribution Good Rate and Distribution Bad Rate: The distribution of good and bad rates across the unique values.\n",
    "    #WoE (Weight of Evidence): A measure of the predictive power of an independent variable in separating the classes.\n",
    "    woe_df = pd.DataFrame(data = lst, columns = ['feature','feature_val','total_ocur','total_churn','total_nochurn' ])\n",
    "\n",
    "    woe_df['share'] = woe_df['total_ocur']/woe_df['total_ocur'].sum()\n",
    "    woe_df['share_churn'] = woe_df['total_churn']/woe_df['total_ocur'].sum()\n",
    "    woe_df['distribution_churn'] = woe_df['total_churn']/woe_df['total_churn'].sum()\n",
    "    woe_df['distribution_nochurn'] = woe_df['total_nochurn']/woe_df['total_nochurn'].sum()\n",
    "    woe_df['WoE'] = np.log(woe_df['distribution_nochurn']/woe_df['distribution_churn'])\n",
    "    woe_df = woe_df.replace({'WoE':{np.inf: 0, -np.inf:0}})\n",
    "    # Calculates the Information Value for each unique value of the feature by multiplying the WoE by the \n",
    "    #difference in distributions of the good and bad rates. The IV is a summary measure that quantifies the\n",
    "    #predictive power of the independent variable.\n",
    "\n",
    "    woe_df['IV'] = woe_df['WoE']*(woe_df['distribution_nochurn'] - woe_df['distribution_churn'])\n",
    "\n",
    "    woe_df = woe_df.sort_values(by = ['feature','feature_val'],ascending = [True, True])\n",
    "\n",
    "    IV = woe_df['IV'].sum()\n",
    "\n",
    "    return IV,woe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to transform a numeric column in bins\n",
    "def quantile_cut(col):\n",
    "    tresholds = np.linspace(0,1,21)\n",
    "    \n",
    "    col_cut = pd.cut(col, bins = np.quantile(col.dropna(),tresholds), include_lowest = True, duplicates='drop')\n",
    "    col_cut = col_cut.cat.add_categories(['NULL'])\n",
    "    col_cut.fillna('NULL', inplace = True)\n",
    "\n",
    "    return col_cut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IV = df.drop('target_class',axis = 'columns').apply(quantile_cut)\n",
    "\n",
    "# target is removed from the quantile cut application then we put back in for the bivariate analysis\n",
    "\n",
    "df_IV.insert(0,'target_class',df['target_class'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
